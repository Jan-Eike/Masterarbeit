\section{Datasets}

We will primarily use two datasets. One for validity classification and one for stance classification. In this chapter will introduce both of them, describe our preprocessing and also mention additional data we are going to use besides these datasets.

\subsection{Validity Dataset}

The dataset for validity classification is taken from the shared task on predicting validity and novelty of arguments \cite{argsvalidnovel2022}. After discarding columns that are irrelevant for our work, each sample consists of the following columns:
\begin{enumerate}
	\item[\textbullet] Topic: Title of the debate
	\item[\textbullet] Premise: Some sentence about the topic
	\item[\textbullet] Conclusion: Conclusion of the premise
	\item[\textbullet] Validity
	\begin{enumerate}
		\item[-] Label 1: Conclusion is valid.
		\item[-] Label 0: Conclusion is defeasibly valid (This label is not contained in the test set. Hence we discard it completely).
		\item[-] Label -1: Conclusion is invalid.
	\end{enumerate}
\end{enumerate}
The data is already split in train, dev and test datasets, with a ratio of roughly $0.5$, $0.15$, $0.35$ and the dev and test set do not share the same topics with the train set. In the training set, about $53\%$ of the data has the validity label 1 and in both dev and test set about $60\%$.

\subsection{Stance Dataset}

The dataset for stance classification is taken from the the IBM project debater datasets \cite{stancedata, ibm}. Again, we discard unused columns, resulting in the following columns:
\begin{enumerate}
	\item[\textbullet] Topic: A statement about an arbitrary topic.
	\item[\textbullet] Argument: Argument regarding the topic.
	\item[\textbullet] Stance: Stance of the argument towards the topic.
	\begin{enumerate}
		\item[-] Label 1: Positive stance.
		\item[-] Label -1: Negative stance.
	\end{enumerate}
\end{enumerate}
The data is split with a ratio of about $0.7$, $0.2$, $0.1$ in train, dev and test data. Both labels roughly have a $50\%$ probability of occurrence.

\subsection{Additional Data}

Since not a lot of samples from both datasets, do not include conclusions or arguments that start with linking word, we decided to pretrain our models on artificially created data that explicitly uses linking words. To generate this data, we used ChatGPT \cite{chatgpt}, using prompts of the form
\begin{center}
	\textit{give me ten arguments consisting of multiple sentences and a conclusion that is indicated with <linking word>}.
\end{center}

Where we replace <linking word> with each of the linking words, we decided to use. 

\subsection{Preprocessing}

Besides the standard BERT preprocessing which consists of splitting strings in sub-word tokens and converting these strings to ids \cite{bertprepro}, we decided to combine all the different columns of the datasets that contain text (e.g. Premise, Conclusion) into one column in the following way:
\begin{enumerate}
	\item Add markers to the start of each column: <\_t> for the topic column, <\_p> for the premise column, <\_c> for the conclusion or argument column.
	\item Concatenate the columns to one text column.
	\item Add punctuation between the columns.
\end{enumerate}

After preprocessing, one data point looks like this:
\begin{center}
\raggedright
"<\_t> TV viewing is harmful to children. <\_p> The popularity of TV watching is among the reasons of this phenomenon. Violence, aggression, crimes and wars are broadcast through the daily news as well as in movies, showing dark pictures that encourage psychological tension, pessimism and negative emotions. <\_c> Depression is a well-known psychological problem of modern society that is partly caused by TV watching."
\end{center}