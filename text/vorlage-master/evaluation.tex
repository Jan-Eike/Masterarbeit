\section{Evaluation}

In this chapter, we will describe all the used model configurations and present their results for both validity classification and stance detection. Afterwards, we will discuss the results and investigate how they line up with our expectations.

\subsection{Model Configurations and Results}

The main structure of all different models is based on the model in Figure \ref{fig:model-architecture_full}. In this section, we will describe the exact configurations of all used models. \\
Since using all possible model configurations we explained would result in more than $2^{10}$ configurations, we decided to not show the steps and results for models that severely under perform compared to other models. These steps include further pretraining with ChatGPT data (\ref{sec:chatgpt}) and using only the linking word embeddings without concatenating them with the LM embeddings. We also decided not to use both ways of generating LM embeddings (\ref{sec:bertembeddings}) since one way always slightly outperforms the other one. For the individual training, we only use the strategy of calculating the average over all token vectors along the first dimension and for the complete training, we only use the [CLS] token as embedding. We will also only use the maximum probability approach for calculating linking word probabilities for vailidity classification as described in the last part of section \ref{sec:linkingembeddings}.  \\
To describe each configuration, we are going to recap all the steps and give them a label.
\begin{itemize}
	\item Used LM $\in \{\text{BERT}, \text{RoBERTa}\}$.
	\item Used LM embedding $\in \{E_{[CLS]}, E_{avg}\}$.
	\item Used approach to calculate linking word probabilities $\in \{P_{max}, P\}$.
	\item Used classifier $\in \{\text{Neural Network Classifier (NN)}, \text{LightGBM Classifier (LGBM)}\}$.
	\item Used set of linking words $\in \{L_1, L_2, L_3, L_4\}$. The entire linking word sets can be found in the appendix.
	\item Used embeddings $\in \{E, E_{Matrix}\}$. With $E_{Matrix}$ being the matrix embeddings and $E$ the normal linking words embeddings (\ref{sec:linkingembeddings}).
	\item Feature Extraction $\in \{\textit{FE}^+, \textit{FE}^-\}$. With $\textit{FE}^+$ meaning with feature extraction and $\textit{FE}^-$ without.
\end{itemize}
With these labels in mind, we present the classification results in two tables, Table \ref{fig:validityresults} corresponding to the results for validity classification and Table \ref{fig:stanceresults} corresponding to the results for stance detection. Each model is trained and evaluated $3$ times and each value in the tables is the corresponding average f1-score. Models with the term "Complete" in the beginning of a row, use the complete training approach and hence $E_{[CLS]}$ for the LM embeddings, the other models use individual training and therefore $E_{avg}$. The models for validity classification will always use $P_{max}$ for the linking word probabilities calculation as described in the beginning of this section. For stance detection, we will always use $P$. \\

\begin{table}[h]
  	\tiny
  	\centering
	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.589 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.580 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.534 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.549 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ & 0.564 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.557 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.552 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.544 & x \\
 			\hline
 			NN + LM only & 0.579 & 1 \\
 			\hline
 			LGBM + LM only & 0.547 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & \textbf{0.754} & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & 0.752 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.746 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.745 & x \\
 			\hline
 			Baseline & 0.767 & 1 \\
 			\hline
 			\hline
 			Average & 0.624 & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_1$.}%
  	\end{subtable}%
  	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.600 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.588 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.519 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.549 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ &  0.550 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.511 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.557 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.521 & x \\
 			\hline
 			NN + LM only & 0.599 & 1 \\
 			\hline
 			LGBM + LM only & 0.532 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & 0.765 & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & 0.765 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.753 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.752 & x \\
 			\hline
 			Baseline & 0.767 & 1\\
 			\hline
 			\hline
 			Average & 0.622 & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_2$.}%
  	\end{subtable}
  	\caption{Evaluation results for validity classification with linking words lists $L_1$ and $L_2$.}%
  	\label{fig:validityresults}
\end{table}

\begin{table}[h]
  	\tiny
  	\centering
	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.611 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.588 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.500 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.549 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ & 0.532 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.547 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.547 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.540 & x \\
 			\hline
 			NN + LM only & 0.579 & 1 \\
 			\hline
 			LGBM + LM only & 0.547 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & 0.769 & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & 0.769 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.753 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.753 & x \\
 			\hline
 			Baseline & 0.767 & 1\\
 			\hline
 			\hline
 			Average & 0.623 & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_3$.}%
  	\end{subtable}%
  	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.583 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.593 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.514 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.504 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ & 0.548 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.521 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.571 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.602 & x \\
 			\hline
 			NN + LM only & 0.600 & 1 \\
 			\hline
 			LGBM + LM only & 0.532 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & \textbf{0.752} & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & \textbf{0.752} & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.748 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.747 & x \\
 			\hline
 			Baseline & 0.767 & 1\\
 			\hline
 			\hline
 			Average & 0.622 & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_4$.}%
  	\end{subtable}
  	\caption{Evaluation results for validity classification with linking words lists $L_3$ and $L_4$.}%
  	\label{fig:validityresults2}
\end{table}

When looking at the results for validity classification, we notice that for configurations that used the individual training approach, the model barely learns anything and would only outperform a random classifier by about $0.05$ in f1-score. Configurations that use the matrix embedding $E_{Matrix}$ tend to perform slightly worse than configurations that do not use them. Also feature selection does not increase the f1-score in most cases. Only the complete training approach can compete with the baseline. Both achieve f1-scores of about $0.75$ - $0.76$. The only configurations that outperform the baseline are 
Complete NN + $E$ + $\textit{FE}^-$ and Complete NN + $E$ + $\textit{FE}^+$ with an f1-score of $0.769$. The results are also pretty much equal along all sets of linking words.

\begin{table}[h]
  	\tiny
  	\centering
	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.896 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.894 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.815 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.827 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ & 0.893 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.891 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.894 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.894 & x \\
 			\hline
 			NN + LM only & 0.895 & 1 \\
 			\hline
 			LGBM + LM only & 0.895 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & 0.886 & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & 0.881 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.433 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.580 & x \\
 			\hline
 			Baseline & 0.908 & 1\\
 			\hline
 			\hline
 			Average & 0.836 & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_1$.}%
  	\end{subtable}%
  	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.898 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.897 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.715 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.806 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ & 0.900 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.898 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.900 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.891 & x \\
 			\hline
 			NN + LM only & 0.900 & 1 \\
 			\hline
 			LGBM + LM only & 0.897 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & 0.851 & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & 0.844 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.450 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.546 & x \\
 			\hline
 			Baseline & 0.908 & 1\\
 			\hline
 			\hline
 			Average & 0.825 & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_2$.}%
  	\end{subtable}
  	\caption{Evaluation results for stance detection with linking words lists $L_1$ and $L_2$.}%
  	\label{fig:stanceresults}
\end{table}

\begin{table}[h]
  	\tiny
  	\centering
	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.589 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.580 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.534 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.549 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ & 0.564 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.557 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.552 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.544 & x \\
 			\hline
 			NN + LM only & 0.579 & 1 \\
 			\hline
 			LGBM + LM only & 0.547 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & \textbf{0.754} & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & 0.752 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.746 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.504 & x \\
 			\hline
 			Baseline & 0.760 & 1\\
 			\hline
 			\hline
 			Average & x & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_3$.}%
  	\end{subtable}%
  	\begin{subtable}{.5\textwidth}
		\centering
  		\renewcommand{\arraystretch}{1.4}
   		\begin{tabular}{|| l || c | c ||}
   			\hline
   			{Configuration} & {BERT} & {RoBERTa} \\
   			\hline\hline
   			NN + $E$ + $\textit{FE}^-$ &  0.583 & x \\
 			\hline
 			NN + $E$ + $\textit{FE}^+$ & 0.593 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.514 & x \\
 			\hline
 			NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.504 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^-$ & 0.548 & x \\
 			\hline
 			LGBM + $E$ + $\textit{FE}^+$ & 0.521 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^-$ & 0.571 & x \\
 			\hline
 			LGBM + $E_{Matrix}$ + $\textit{FE}^+$ & 0.602 & x \\
 			\hline
 			NN + LM only & 0.600 & 1 \\
 			\hline
 			LGBM + LM only & 0.532 & 1 \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^-$ & \textbf{0.752} & x \\
 			\hline
 			Complete NN + $E$ + $\textit{FE}^+$ & \textbf{0.752} & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ & 0.502 & x \\
 			\hline
 			Complete NN + $E_{Matrix}$ + $\textit{FE}^+$ & 0.502 & x \\
 			\hline
 			Baseline & 0.760 & 1\\
 			\hline
 			\hline
 			Average & x & x \\
 			\hline
		\end{tabular}
		\renewcommand{\arraystretch}{1}
  		\caption{Linking word list $L_4$.}%
  	\end{subtable}
  	\caption{Evaluation results for stance detection with linking words lists $L_3$ and $L_4$.}%
  	\label{fig:stanceresults2}
\end{table}

We can already see that the models achieve much higher f1-scores for stance detection, even when trained with the individual training approach. Except for the configurations Complete NN + $E_{Matrix}$ + $\textit{FE}^-$ and Complete NN + $E_{Matrix}$ + $\textit{FE}^+$, most models achieved an f1-score of more than $0.8$ and a lot of them even more than $0.89$. This however is still worse than the baseline by about $0.008$ for the best performing configuration. We also still cannot see a noticeable difference between the sets of linking words, neither in the meaning of the words nor in the number of used words. Another difference we can observe it that using $E_{Matrix}$ decreases the performance of configurations that use a neural network classifier but for the LightGBM classifier it does not seem to make a difference.

\subsection{Discussion of the Results}
\textbf{Validity Classification} \\
When recalling the motivation of this thesis, we will primarily look at the models that used the complete training approach since for individual training, the results are just not good enough to make statement regarding the effect of linking words. But when it comes to model that were trained with the complete training approach, we can make comparisons between the effects of the four different sets of linking words. Unfortunately, the differences in the average f1-score for configurations with the complete training approach, is only about $0.016$ between $L_1$ which has the lowest and $L_3$ which has the highest f1-score. This difference alone does not really give us any insight to whether or not the actual meaning of the words in one of these lists matters. The f1-score also does not really differ from the baseline score hence it seems that also just adding embeddings for arbitrary words does not increase the performance of the LM. Even if we ignore the fact that each configurations was only trained and evaluated $3$ times and hence randomness is still quite a big factor, a difference of at most $0.016$ in f1-score is not significant enough to make any assumptions about these results. \\
We can however discuss, what the reasons for the results might be. One possible reason is the aforementioned lack of a bigger dataset but we will see how a bigger dataset performs when we discuss the results for stance detection. The other possible reason will be discussed after discussing the results for stance detection. \\

\textbf{Stance detection} \\
Since we can see a drastic improvement in f1-score for the models that were trained using the individual training approach, this indeed might have been a problem of not enough data for a good quality training. Unfortunately however, we can still not make any statements about our motivation in the beginning since the f1-scores for most other configurations are very close to each other, regardless of the used set of linking words. These results lead us to the main possible reason for the underwhelming results and for the lack of improvement using our approach for both classification tasks. The reason might be that the linking word embeddings were also calculated by using the same language model that the LM embeddings were extracted from (e.g. We concatenated BERT embeddings with linking word embeddings that were calculated using the same BERT model for masking). For that reason, the embeddings most likely do not contain any information that the LM embeddings do not already contain (no \textit{new} information is added to the LM embedding). So all in all the embedding just gets bigger but does not actually contain more information about the current data point. Another reason could also be that linking words are not really used that much in spoken language but rather in formal texts and therefore, language models might not give these words enough weighting to be relevant enough for our problems.


https://7esl.com/linking-words/