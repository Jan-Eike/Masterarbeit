\section{Additional Linking Word Lists}

Our used $90$ linking words, taken from \cite{linking}. We only used phrased consisting of one word.

$L_3$ = [
		accordingly,
        consequently,
        hence,
        then,
        therefore,
        thus,
        absolutely,
        chiefly,
        clearly,
        definitely,
        especially,
        even,
        importantly,
        indeed,
        naturally,
        never",
        obviously,
        particularly,
        positively,
        surprisingly,
        truly,
        undoubtedly,
        additionally,
        also,
        and,
        besides,
        finally,
        first,
        further,
        furthermore,
        last,
        "Moreover",
        "Second",
        "Third",
        "Too",
        "Including",
        "Like",
        "Namely",
        "Specifically",
        "Alternatively",
        "Conversely",
        "However",
        "Instead",
        "Nevertheless",
        "Nonetheless",
        "Nor",
        "Notwithstanding",
        "Rather",
        "Though",
        "Unlike",
        "Whereas",
        "While",
        "Yet",
        "Alike",
        "Both",
        "Either",
        "Equal",
        "Equally",
        "Likewise",
        "Resembles",
        "Similarly",
        "Altogether",
        "Briefly",
        "Overall",
        "Ultimately",
        "As",
        "If",
        "Since",
        "Unless",
        "When",
        "Whenever",
        "Lest",
        "Concerning",
        "Considering",
        "Regarding",
        "Reiterated",
        "Regularly",
        "Typically",
        "Mostly",
        "Normally",
        "Often",
        "Commonly",
        "because",
        "although",
        "but",
        "still",
        "for",
        "so",
        "anyway",
        "Mainly"
]

The $90$ most common English words according to \cite{mostcommon}.

$L_4$ = [the,
        be,
        to,
        of,
        and,
        a,
        in,
        that,
        have,
        I,
        it,
        for,
        not,
        on,
        with,
        he,
        as,
        you,
        do,
        at,
        this,
        but,
        his,
        by,
        from,
        they,
        we,
        say,
        her,
        she,
        or,
        an,
        will,
        my,
        one,
        all,
        would,
        there,
        their,
        what,
        so,
        up,
        out,
        if,
        about,
        who,
        get,
        which,
        go,
        me,
        when,
        make,
        can,
        like,
        time,
        no,
        just,
        him,
        know,
        take,
        person,
        into,
        year,
        your,
        good,
        some,
        could,
        them,
        see,
        other,
        than,
        then,
        now,
        look,
        only,
        come,
        its,
        over,
        think,
        also,
        back,
        after,
        use,
        two,
        how,
        our,
        work,
        first,
        well,
        way]

\section{Discarded Ideas}
We tried a vast amount of ideas to improve the performance of our models. We discarded a lot of them after evaluating them and getting subpar results. In this section, we are going to list a few of these ideas:

\begin{itemize}
	\item Calculating the perplexity of the model depending on the seen linking word at the beginning of a conclusion/argument 			  for each linking word. This was then used as the linking word embedding or extended the embedding.
	\item Using different classifiers like SVMs or a nearest neighbor classifier using cosine distance.
	\item Standardizing the embeddings to have mean $0$ and standard deviation $1$.
\end{itemize} 

As already mentioned in the evaluation, we decided not to show the results of some ideas that still had good results, due to the fact that they were too similar to presented results. Examples for that are additional pretraining and using RoBERTa instead of BERT. Another discarded idea is using only the linking words for classification instead of combining them with the LM embeddings. We will however show some of these results here as mentioned in the evaluation since they give nice insights on why we decided to use matrix embeddings and LGBM.

\section{Additional Results}