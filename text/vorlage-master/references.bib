@inproceedings{argsvalidnovel2022,
    title = "Overview of the 2022 Validity and Novelty Prediction Shared Task",
    author = "Heinisch, Philipp  and
      Frank, Anette  and
      Opitz, Juri  and
      Plenz, Moritz  and
      Cimiano, Philipp",
    booktitle = "Proceedings of the 9th Workshop on Argument Mining",
    month = oct,
    year = "2022",
    address = "Online and in Gyeongju, Republic of Korea",
    publisher = "International Conference on Computational Linguistics",
    url = "https://aclanthology.org/2022.argmining-1.7",
    pages = "84--94",
    abstract = "This paper provides an overview of the Argument Validity and Novelty Prediction Shared Task that was organized as part of the 9th Workshop on Argument Mining (ArgMining 2022). The task focused on the prediction of the validity and novelty of a conclusion given a textual premise. Validity is defined as the degree to which the conclusion is justified with respect to the given premise. Novelty defines the degree to which the conclusion contains content that is new in relation to the premise. Six groups participated in the task, submitting overall 13 system runs for the subtask of binary classification and 2 system runs for the subtask of relative classification. The results reveal that the task is challenging, with best results obtained for Validity prediction in the range of 75{\%} F1 score, for Novelty prediction of 70{\%} F1 score and for correctly predicting both Validity and Novelty of 45{\%} F1 score. In this paper we summarize the task definition and dataset. We give an overview of the results obtained by the participating systems, as well as insights to be gained from the diverse contributions.",
}

@article{mohammad2017stance,
  title={Stance and sentiment in tweets},
  author={Mohammad, Saif M and Sobhani, Parinaz and Kiritchenko, Svetlana},
  journal={ACM Transactions on Internet Technology (TOIT)},
  volume={17},
  number={3},
  pages={1--23},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@misc{languagemodels2023,
  title={N-gram Language Models},
  author={Daniel Jurafsky, James H. Martin},
  year={2023},
  note={accessed 14-August-2023},
  howpublished="\url{https://web.stanford.edu/~jurafsky/slp3/3.pdf}"
}

@article{bert,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype   = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{roberta,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype   = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lgbm,
 author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{lgbmpython,
	author={Microsoft},
	title={LightGBM},
	howpublished={\url{https://lightgbm.readthedocs.io/en/latest/Python-Intro.html}},
	note={accessed 22-October-2023; Python package for LightGBM.}
}

@article{shap,
  author       = {Scott M. Lundberg and
                  Su{-}In Lee},
  title        = {A unified approach to interpreting model predictions},
  journal      = {CoRR},
  volume       = {abs/1705.07874},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.07874},
  eprinttype    = {arXiv},
  eprint       = {1705.07874},
  timestamp    = {Fri, 26 Nov 2021 16:33:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LundbergL17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{gradboost,
  author={Jason Brownlee},
  title={A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning},
  year=2016,
  note={accessed 14-August-2023},
  howpublished={\url{https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/}}
}

@article{metrics,
  author = {Hossin, Mohammad and M.N, Sulaiman},
  year = {2015},
  month = {03},
  pages = {01-11},
  title = {A Review on Evaluation Metrics for Data Classification Evaluations},
  volume = {5},
  journal = {International Journal of Data Mining \& Knowledge Management Process},
  doi = {10.5121/ijdkp.2015.5201}
}

@misc{confusion,
  title={Was ist die Konfusionsmatrix?},
  year=2022,
  howpublished={\url{https://databasecamp.de/ki/konfusionsmatrix#:~:text=positiven%20Schnelltest%20haben.-,Was%20ist%20eine%20Confusion%20Matrix%3F,man%20das%20Testset%20des%20Datensatzes.}}, 
  note={accessed 14-August-2023}
}

@article{stancedata,
  author       = {Shai Gretz and
                  Roni Friedman and
                  Edo Cohen{-}Karlik and
                  Assaf Toledo and
                  Dan Lahav and
                  Ranit Aharonov and
                  Noam Slonim},
  title        = {A Large-scale Dataset for Argument Quality Ranking: Construction and
                  Analysis},
  journal      = {CoRR},
  volume       = {abs/1911.11408},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.11408},
  eprinttype   = {arXiv},
  eprint       = {1911.11408},
  timestamp    = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-11408.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ibm,
  author = {IBM},
  title = {Project Debater Datasets},
  howpublished={\url{https://research.ibm.com/haifa/dept/vst/debating_data.shtml#Argument_Quality}},
  note={accessed 16-August-2023}
}

@misc{chatgpt,
  author={Open AI},
  title={Introducing ChaptGPT},
  howpublished={\url{https://openai.com/blog/chatgpt}},
  note={accessed 16-August-2023}
}

@misc{bertprepro,
  author={Huggingface},
  title={Tokenizer},
  howpublished={\url{https://huggingface.co/docs/transformers/main_classes/tokenizer}},
  note={accessed 16-August-2023}
}

@misc{berttraining,
  author={Huggingface},
  title={Text Classification},
  howpublished={\url{https://huggingface.co/docs/transformers/tasks/sequence_classification}},
  note={accessed 29-August-2023}
}

@misc{bertmask,
  author={Huggingface},
  title={BERT},
  howpublished={\url{https://huggingface.co/docs/transformers/v4.32.1/en/model_doc/bert#transformers.BertForMaskedLM}},
  note={accessed 30-August-2023}
}

@misc{transformers,
  author={Huggingface},
  title={Huggingface Transformers},
  howpublished={\url{https://huggingface.co/docs/transformers/v4.32.1/en/index}},
  note={accessed 03-November-2023}
}

  
@misc{bertimpl,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    howpublished = {\url{https://aclanthology.org/2020.emnlp-demos.6}},
    doi = "\url{10.18653/v1/2020.emnlp-demos.6}",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",
}

@misc{lgbmimportance,
	author={Microsoft},
	title={LightGBM Feature Importance},
	howpublished={\url{https://github.com/microsoft/LightGBM/blob/master/python-package/lightgbm/basic.py\#L4491}},
	note={accessed 19-September-2023; Method for feature importance}
}

@article{Discours1999,
title = {What are discourse markers?},
journal = {Journal of Pragmatics},
volume = {31},
number = {7},
pages = {931-952},
year = {1999},
note = {Pragmatics: The Loaded Discipline?},
issn = {0378-2166},
doi = {https://doi.org/10.1016/S0378-2166(98)00101-5},
url = {https://www.sciencedirect.com/science/article/pii/S0378216698001015},
author = {Bruce Fraser},
abstract = {This paper is an attempt to clarify the status of discourse markers. These lexical expressions have been studied under various labels, including discourse markers, discourse connectives, discourse operators, pragmatic connectives, sentence connectives, and cue phrases. Although most researchers agree that they are expressions which relate discourse segments, there is no agreement on how they are to be defined or how they function. After reviewing prior theoretical research, I define discourse markers as a class of lexical expressions drawn primarily from the syntactic classes of conjunctions, adverbs, and prepositional phrases. With certain exceptions, they signal a relationship between the interpretation of the segment they introduce, S2, and the prior segment, S1. They have a core meaning, which is procedural, not conceptual, and their more specific interpretation is ‘negotiated’ by the context, both linguistic and conceptual. There are two types: those that relate the explicit interpretation conveyed by S2 with some aspect associated with the segment, S1; and those that relate the topic of S2 to that of S1. I conclude by presenting what appears to be the major classes according to their function.}
}

@misc{bertbaseuncased,
	author={Huggingface},
	title={bert-base-uncased},
	howpublished={\url{https://huggingface.co/bert-base-uncased}},
	note={accessed 22-October-2023; BERT-base-uncased model from huggingface.}
}

@misc{robertabase,
	author={Huggingface},
	title={roberta-base},
	howpublished={\url{https://huggingface.co/roberta-base}},
	note={accessed 03-November-2023; roberta-base model from huggingface.}
}

@article{gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bertexplain,
  author       = {Ian Tenney and
                  Dipanjan Das and
                  Ellie Pavlick},
  title        = {{BERT} Rediscovers the Classical {NLP} Pipeline},
  journal      = {CoRR},
  volume       = {abs/1905.05950},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.05950},
  eprinttype    = {arXiv},
  eprint       = {1905.05950},
  timestamp    = {Tue, 28 May 2019 12:48:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-05950.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{pytorch,
  author       = {Adam Paszke and
                  Sam Gross and
                  Francisco Massa and
                  Adam Lerer and
                  James Bradbury and
                  Gregory Chanan and
                  Trevor Killeen and
                  Zeming Lin and
                  Natalia Gimelshein and
                  Luca Antiga and
                  Alban Desmaison and
                  Andreas K{\"{o}}pf and
                  Edward Z. Yang and
                  Zach DeVito and
                  Martin Raison and
                  Alykhan Tejani and
                  Sasank Chilamkurthy and
                  Benoit Steiner and
                  Lu Fang and
                  Junjie Bai and
                  Soumith Chintala},
  title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  journal      = {CoRR},
  volume       = {abs/1912.01703},
  year         = {2019},
  url          = {\url{http://arxiv.org/abs/1912.01703}},
  eprinttype    = {arXiv},
  eprint       = {1912.01703},
  timestamp    = {Tue, 02 Nov 2021 15:18:32 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-01703.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{mostcommon,
	author={EnglishClub},
	title={100 Most Common Words},
	howpublished={\url{https://www.englishclub.com/vocabulary/common-words-100.php}},
	note={accessed 12-November-2023}
}

@misc{linking,
	author={7ESL},
	title={Linking Words, Connecting Words: Full List and Useful Examples},
	howpublished={\url{https://7esl.com/linking-words/}},
	note={accessed 12-November-2023}
}

@article{conceptnet,
  author       = {Robyn Speer and
                  Joshua Chin and
                  Catherine Havasi},
  title        = {ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
  journal      = {CoRR},
  volume       = {abs/1612.03975},
  year         = {2016},
  url          = {http://arxiv.org/abs/1612.03975},
  eprinttype    = {arXiv},
  eprint       = {1612.03975},
  timestamp    = {Sat, 23 Jan 2021 01:19:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/SpeerCH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rgt,
  author       = {Shangbin Feng and
                  Zhaoxuan Tan and
                  Rui Li and
                  Minnan Luo},
  title        = {Heterogeneity-aware Twitter Bot Detection with Relational Graph Transformers},
  journal      = {CoRR},
  volume       = {abs/2109.02927},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.02927},
  eprinttype    = {arXiv},
  eprint       = {2109.02927},
  timestamp    = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-02927.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{shgn,
  author       = {Qingsong Lv and
                  Ming Ding and
                  Qiang Liu and
                  Yuxiang Chen and
                  Wenzheng Feng and
                  Siming He and
                  Chang Zhou and
                  Jianguo Jiang and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {Are we really making much progress? Revisiting, benchmarking, and
                  refining heterogeneous graph neural networks},
  journal      = {CoRR},
  volume       = {abs/2112.14936},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.14936},
  eprinttype    = {arXiv},
  eprint       = {2112.14936},
  timestamp    = {Wed, 15 Mar 2023 20:26:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-14936.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}