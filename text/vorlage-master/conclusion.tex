\section{Conclusion and Future Work}
The motivation for this thesis was to evaluate whether or not additional features, created based on the use of linking words to indicate for example conclusions, can improve classifiers. We used masked language models to calculate probabilities for these linking words to indicate a conclusion or an argument and built word vector embeddings with these probabilities to extend already existing LM embeddings used for classification. We tried to build these embeddings in various ways, used different classifiers, language models, sets of linking words and training procedures. In the end, we were not able to improve our baseline, consisting of a simple LM classifier (e.g. BERT), even though the method of extending embeddings is commonly used to improve performance. We argued a possible reason for this might be the lack of new information coming from our embeddings, since the probabilities used in them, we also given by a LM. This could be investigated further, by using techniques that are not based on language models to integrate linking words into a classification task. Another approach could be to augment the data with these linking words instead of using them in the model itself. All in all, there is still a lot that can be experimented with regarding this topic but this thesis does not indicate that linking words alone play a big enough role in the way language models understand language to increase their performance using more explicit information about these words.